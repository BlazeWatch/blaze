{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 18:28:33.510541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 18:28:34.107181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-08 18:28:38.915342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:38.938551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:38.938588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:38.939629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:38.939686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:38.939709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:41.158711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:41.158764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:41.158774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-08 18:28:41.158796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-08 18:28:41.158825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8351 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./blaze_nlp\")\n",
    "lstm_model = keras.models.load_model(\"lstm_model.h5\")\n",
    "scaler = pickle.load(open(\"scaler.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def derive_sentiment(model_output: {\"label\": str, \"score\": float}) -> float:\n",
    "    \"\"\"\n",
    "    Derives the sentiment score from the model's output.\n",
    "\n",
    "    :param model_output: A dictionary containing the model's classification label and score.\n",
    "    :return: A sentiment score based on the model's output. Positive for 'yes_fire', negative for 'no_fire'.\n",
    "    \"\"\"\n",
    "\n",
    "    return model_output[\"score\"] * (1 if model_output[\"label\"] == \"yes_fire\" else -1)\n",
    "\n",
    "\n",
    "def predict_fire_from_temp(temperatures: Union[list[int], list[list[int]]]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Predicts fire based on the given temperatures.\n",
    "\n",
    "    :param temperatures: A list or list of lists containing temperature values.\n",
    "    :return: A list of dictionaries containing the prediction labels ('no_fire' or 'yes_fire') and scores.\n",
    "    \"\"\"\n",
    "\n",
    "    temperatures = np.array([temperatures]).reshape((-1, 7))\n",
    "    temperatures = scaler.transform(temperatures)\n",
    "    temperatures = temperatures.reshape(\n",
    "        (temperatures.shape[0], temperatures.shape[1], 1)\n",
    "    )\n",
    "\n",
    "    # predict the fire\n",
    "    predictions = lstm_model.predict(temperatures, verbose=0)\n",
    "    output = []\n",
    "    for prediction in list(predictions):\n",
    "        prediction_result = np.argmax(prediction)\n",
    "        label = [\"no_fire\", \"yes_fire\"][prediction_result]\n",
    "        score = prediction[prediction_result]\n",
    "\n",
    "        output.append({\"label\": label, \"score\": score})\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def predict_fire(\n",
    "    tweets: Union[list[str], list[list[str]]],\n",
    "    temperatures: Union[list[int], list[list[int]]],\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Predicts fire based on the given tweets and temperatures.\n",
    "\n",
    "    :param tweets: A list or list of lists containing tweets. Can be an empty list.\n",
    "    :param temperatures: A list or list of lists containing temperature values.\n",
    "    :return: A list of sentiment scores combining the information from tweets and temperatures.\n",
    "             If tweets is an empty list, the function will return predictions based solely on temperatures.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(temperatures[0], list):\n",
    "        temperatures = [temperatures]\n",
    "\n",
    "    if not tweets:\n",
    "        # Handle case when tweets is an empty list\n",
    "        flat_temperatures = [temp for temp_batch in temperatures for temp in temp_batch]\n",
    "        temperature_fire = predict_fire_from_temp(flat_temperatures)\n",
    "        return [derive_sentiment(temp) for temp in temperature_fire]\n",
    "\n",
    "    if len(tweets) and not isinstance(tweets[0], list):\n",
    "        tweets = [tweets]\n",
    "\n",
    "    # Flattening tweets and storing their batch indices\n",
    "    flat_tweets = []\n",
    "    tweet_batch_indices = [0]\n",
    "    for tweet_batch in tweets:\n",
    "        flat_tweets.extend(tweet_batch)\n",
    "        tweet_batch_indices.append(len(flat_tweets))\n",
    "\n",
    "    # Flattening temperatures and storing their batch indices\n",
    "    flat_temperatures = [temp for temp_batch in temperatures for temp in temp_batch]\n",
    "    temperature_batch_indices = [0]\n",
    "    for i in range(len(temperatures)):\n",
    "        temperature_batch_indices.append(\n",
    "            temperature_batch_indices[-1] + len(temperatures[i])\n",
    "        )\n",
    "\n",
    "    # Get predictions for the flattened tweets and temperatures\n",
    "    tweet_fire = classifier(flat_tweets)\n",
    "    temperature_fire = predict_fire_from_temp(flat_temperatures)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    # Process predictions based on indices\n",
    "    for i in range(len(temperatures)):\n",
    "        tweet_batch_start, tweet_batch_end = (\n",
    "            tweet_batch_indices[i],\n",
    "            tweet_batch_indices[i + 1],\n",
    "        )\n",
    "        tweet_batch_result = tweet_fire[tweet_batch_start:tweet_batch_end]\n",
    "        temperature_batch_result = temperature_fire[i]\n",
    "\n",
    "        average_tweet_sentiment = 0\n",
    "        for tweet in tweet_batch_result:\n",
    "            sentiment = derive_sentiment(tweet)\n",
    "            average_tweet_sentiment += sentiment * (0.2 if sentiment < 0 else 1)\n",
    "        average_tweet_sentiment = average_tweet_sentiment / len(tweet_batch_result)\n",
    "        average_tweet_sentiment = round(average_tweet_sentiment)\n",
    "\n",
    "        temperature_sentiment = derive_sentiment(temperature_batch_result)\n",
    "\n",
    "        if not average_tweet_sentiment:\n",
    "            output.append(temperature_sentiment)\n",
    "        else:\n",
    "            output.append(average_tweet_sentiment * 0.4 + temperature_sentiment * 0.6)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999906301498414, -1.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fire(\n",
    "    [\n",
    "        [\"i hate this town\", \"look at that forest fire go!\", \"wow that's crazy\"],\n",
    "        [\"asdfsad\", \"asfdasdf\"]\n",
    "    ],\n",
    "    [\n",
    "        [155, 155, 155, 155, 155, 155, 155],\n",
    "        [1, 2, 3, 4, 5, 6, 7]\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.19999999999999996]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impossible case in real life, low temperature but tweets suggesting fire\n",
    "predict_fire(\n",
    "    [\"i hate my life\", \"look at that forest fire go!\", \"wow that's crazy\"],    \n",
    "    [\n",
    "        [1, 2, 3, 4, 5, 6, 7]\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999843835830688]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sometimes tweets are unrelated to the fire, so the algorithm favors temperature data\n",
    "predict_fire(\n",
    "    [\"asdfsad\", \"asfdasdf\"],\n",
    "    [\n",
    "        [155, 155, 155, 155, 155, 155, 155]\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999843835830688]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Works even if there are no tweets that day\n",
    "predict_fire(\n",
    "    [],\n",
    "    [\n",
    "        [155, 155, 155, 155, 155, 155, 155]\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
